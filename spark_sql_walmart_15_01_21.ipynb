{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Start a simple Spark Session\n",
    "spark = SparkSession.builder\\\n",
    "                .master(\"local[*]\")\\\n",
    "                .appName(\"Walmart_Stock\")\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Load the Walmart Stock CSV File\n",
    "df = spark.read\\\n",
    "        .option(\"header\",True)\\\n",
    "        .csv(\"walmart_stock.csv\")\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 schema look?\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|            HV_Ratio|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 New column HV_Ratio =Ratio High price/ Volume of stock per day\n",
    "df2= df.withColumn(\"HV_Ratio\", col(\"High\")/col(\"Volume\"))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 What Day had the peak high price?\n",
    "df.orderBy(col(\"High\").desc())\\\n",
    "    .select(col(\"Date\"))\\\n",
    "    .head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2015-01-13|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6 What Day had the peak high price? SQL\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\" \n",
    "select Date \n",
    "from stock\n",
    "order By High desc \"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       mean_Close|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7 what is the mean of the Close column?\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\"\n",
    "select avg(Close) as mean_Close\n",
    "from stock \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7 what is the mean of the Close column?\n",
    "df.select(col(\"Close\")).agg(avg(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|   10010500|    9994400|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8 What is the max and min of the volumn column?\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\"\n",
    "select min(Volume), max(Volume)\n",
    "from stock \n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|   10010500|    9994400|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8 What is the max and min of the volumn column? (spark)\n",
    "df.select(col(\"Volume\")).agg(min(\"Volume\"), max(\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|jour_close_inf_60|\n",
      "+-----------------+\n",
      "|               81|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9 How many days was the CLose lower than 60 dollars?\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\"\n",
    "select count(Close)  as jour_close_inf_60\n",
    "from stock\n",
    "where Close < 60\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9) How many days was the Close lower than 60 dollars? (spark)\n",
    "df.filter(df['Close'] < 60).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      Pourcentage|\n",
      "+-----------------+\n",
      "|8.426073131955485|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10 percentage of the time was the High greater than 80 dollars \n",
    "spark.sql(\"\"\" \n",
    "select count(Date)*100/(select count(*) from stock) as Pourcentage\n",
    "from stock\n",
    "where High > 80\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.426073131955485"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 percentage of the time was the High greater than 80 dollars (spark)\n",
    "df.filter('High > 80').count() * 100/df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year|max(High)|\n",
      "+----+---------+\n",
      "|2012|77.599998|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2015|90.970001|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 Max high per year\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT substr(Date,1,4) as year, max(High )\n",
    "FROM stock\n",
    "group by year\n",
    "order by year\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year(date)|max(high)|\n",
      "+----------+---------+\n",
      "|      2012|77.599998|\n",
      "|      2013|81.370003|\n",
      "|      2014|88.089996|\n",
      "|      2015|90.970001|\n",
      "|      2016|75.190002|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 Max high per year (Spark)\n",
    "df.groupby(year(\"date\")).agg(max(\"high\")).orderBy(year(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|month|avg_close_per_month|\n",
      "+-----+-------------------+\n",
      "|   01|  71.44801958415842|\n",
      "|   02|    71.306804443299|\n",
      "|   03|  71.77794377570092|\n",
      "|   04|  72.97361900952382|\n",
      "|   05|  72.30971688679247|\n",
      "|   06|   72.4953774245283|\n",
      "|   07|  74.43971943925233|\n",
      "|   08|  73.02981855454546|\n",
      "|   09|  72.18411785294116|\n",
      "|   10|  71.57854545454543|\n",
      "|   11|   72.1110893069307|\n",
      "|   12|  72.84792478301885|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12  average Close for each calendar Month\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT substr(Date,6,2) as month, avg(Close ) as avg_close_per_month\n",
    "FROM stock\n",
    "group by month\n",
    "order by month\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|month(date)|       avg(Close)|\n",
      "+-----------+-----------------+\n",
      "|          1|71.44801958415842|\n",
      "|          2|  71.306804443299|\n",
      "|          3|71.77794377570092|\n",
      "|          4|72.97361900952382|\n",
      "|          5|72.30971688679247|\n",
      "|          6| 72.4953774245283|\n",
      "|          7|74.43971943925233|\n",
      "|          8|73.02981855454546|\n",
      "|          9|72.18411785294116|\n",
      "|         10|71.57854545454543|\n",
      "|         11| 72.1110893069307|\n",
      "|         12|72.84792478301885|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12  average Close for each calendar Month (Spark)\n",
    "df.groupby(month(\"date\")).agg(avg(\"Close\")).orderBy(month(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
